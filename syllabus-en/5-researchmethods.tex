\chapter{Research methods}
\label{ch:researchmethods}

In this chapter you will find recommendations related to some commonly used research methods. More specifically, we discuss the approach of a comparative study, how to correctly set up experiments and how to report correctly on the results obtained (in particular figures).

% Belang van correcte methodologie
%
% Voorbeelden:
% - enquêtes (verwijzen naar Saunders?)
%     - pitfalls: respons, slechte vragen, slechte verwerking, te laat opgengesteld
%     - onderzoekskader, steekproefmethode (benader aselecte steekproef)
%     - stel vragen zo dat je op zoek kan gaan naar verbanden!
%     - tip: neem deel aan enquêtes, bv. van iMinds => voeling met vraagstelling
% - interviews (kwalitatief)
%     - wanneer? Casus leren kennen, vakexpert, requirements-analyse
%     - Goed voorbereiden, opnemen, transcriptie uitschrijven, verwerken
% - experiment
% - vergelijkende studie
% - casus/case study
% - doorlichting beveiliging: risico-analyse Chris Jackson, Network Security Auditing. Cisco Press. 2010.
%
% wanneer pas je elk toe?
% Vaak heb je een combinatie van deze technieken nodig om je onderzoeksvra(a)g(en) goed en onderbouwd te kunnen beantwoorden.

\section{The comparative study}
\label{sec:comparativestudy}

A topic that is often chosen for a bachelor's thesis is a comparative study. You are looking for a solution for a problem in the form of a software or hardware product, platform, service, etc. The aim of the study is to compare all possible alternatives and to make a choice for the most suitable one.

Experience shows that a comparative study is only really good if there is a concrete goal, a real situation where the selected solution will actually be applied. There is a threat that the study will be limited to listing a few arbitrarily chosen options. A section of explanation follows, sometimes simply taken from Wikipedia, with a summary of some advantages and disadvantages, but not structured and without a common thread. A certain aspect such as ``user-friendliness'' is then discussed in one product, but not for the other, etc. Your own input is then minimal: a day's search on Wikipedia, summarize or further write it out, done. This is therefore insufficient.

But how \emph{do} you go about then?

Let's assume that you want to work as a web developer after graduation, and you are looking for a suitable PHP framework to build your websites with.

\subsection{Requirements-analysis}
\label{ssec:requirements-analysis}

To make a good choice, start by collecting \emph{requirements}, both \textit{functional} and \emph{non-functional}, e.g.:

\begin{itemize}
\item Functional requirements
  \begin{itemize}
 \item HTML5/CSS3 support
 \item Responsive design
 \item It must contain an authentication module that supports OpenID, Facebook and Google authentication
  \item \ldots
  \end{itemize}
\item Non-functional requirements
  \begin{itemize}
 \item Must be able to withstand OWASP's top-10 vulnerabilities\footnote{\url{https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project}}
 \item Passwords are stored according to the state-of-the-art (\emph{salted} and \emph{hashed})
 \item Must be open source
 \item Must be free
  \item \ldots
  \end{itemize}
\end{itemize}

If you are doing the research for a ``client'' (i.e. your co-promoter), then you naturally involve all stakeholders in this process! You list the requirements and divide them according to importance, for example using the MoSCoW technique~\parencite{Nordenstam2014}. You then divide the requirements into categories such as ``must-have'', ``should-have'' and ``nice-to-have''.

\subsection{Long list}
\label{ssec:long-list}

Then you look for \emph{as many as possible} alternatives that are eligible to be used, i.e. ~all the ones you can find. You mention them in this \emph{long list} (which can sometimes consist of dozens of alternatives) by name, possibly mentioning a website and a description in one sentence. You check each alternative against the requirements, if this is possible already based on the information that you find on the website or from other sources. You just leave things you can't verify to check later or maybe even ignore them (e.g. if it's an insignificant feature, or if several other must-haves aren't met). You then sort the long list according to the number of requirements met, and make a clear table of this. Hopefully you've been left with some alternatives that meet all the must-haves and as many should-haves and nice-to-haves as possible.

\subsection{Short list and proof-of-concept (POC)}
\label{ssec:short-list-poc}

Keep the most promising alternatives for the next phase. You will discuss the alternatives in this \emph{short list} in more detail and compare them further. If necessary, set up a \emph{proof-of-concept} (see Section \ref{sec:poctestsetup}), in which you try out one or more of the most promising alternatives based on the same defined \emph{scenario} in which you discuss the requirements that you have not yet been able to verify.

\subsection{Conclusion}
\label{ssec:comparativestudyconclusion}

Finally, you give your \emph{recommendation}, the alternative that best meets the requirements, and what else needs to be done to make it even more suitable.

\section{Proof-of-concept or test setup}
\label{sec:poctestsetup}

With a proof-of-concept or test setup you want to demonstrate that a proposed or promising solution for the research question can also be realized in practice. It is often necessary to set up different variants next to each other, so that you can still compare.

A good test setup satisfies three properties \autocite{Liberman2015}:

\begin{description}
  \item[Reproducibility] be able to rebuild the test setup (possibly by an independent party) and run the analysis again so that you can verify that you get similar results.
  \item[Replicability] is similar to the above, but expressed more strongly: with a replicable test set-up it is possible that an independent party can repeat this \textit{exact} setup and obtain the same results.
  \item[Reusability] can set up variants of the test environment so that they can be compared.
\end{description}

The best way to achieve this is to automate the process.

\subsection{Reproducibility}
\label{ssec:reproducibility}

A minimum expectation of a bachelor's thesis text is that the reader must be able to reproduce the test set-up on the basis of the description and must be able to verify your conclusions. That also means that your description must be sufficiently detailed. At the beginning of this section, give a clear picture in your text with a schema of the set up test environment. Then describe in detail:

\begin{itemize}
 \item What hardware was used? Computers, network equipment, other ICT infrastructure. What specifications do these devices have (CPU, memory, hard drive type and size, network interface speed, etc.)?
 \item Which software packages are installed that are relevant for the test setup? Which specific editions and/or version numbers?
 \item How exactly did the installation procedure go? Which settings have been adjusted, and how are all components configured?
\end{itemize}

\subsection{Replicability}
\label{ssec:replicability}

It is important for both you and the reader of your bachelor's thesis to automate the setting up of the test environment as much as possible, for example by writing an installation script. In this way you achieve the \textit{replicability} of your test setup.

For many situations you can develop a proof-of-concept with virtual machines. In this way you create a protected environment without having to install additional software on your physical system. Vagrant\footnote{\url{https://www.vagrantup.com/}} is an interesting tool to set up fully automated reproducible virtual test environments. It is not itself a virtualization system, but it addresses VirtualBox, VMWare or Hyper-V from the command-line. You describe in a configuration file (\texttt{Vagrantfile}) which VMs you need, with how many processor cores/memory/disk space, with which operating system, etc. Via a script or configuration management system (such as Ansible) you can determine the precise configuration of the Describe VMs (software installation, users, network services, configuration files, etc.). The code/configuration to set up a Va\-grant-\ environment is very compact, completely text-based and thus can be maintained in a version control system.

Docker\footnote{\url{https://docker.com/}} is also an interesting tool for this. Docker is a form of so-called container virtualization. Virtual machines (containers) do not have their own OS, but in principle only contain an application. The OS of the physical system is reused by the containers. This makes containers a lot more efficient in terms of disk and memory usage for the physical system. However, there are a number of limitations that make Docker not always useful as a platform for a test setup.

If you notice that you have made a mistake in the configuration of your test setup, you can adjust the installation script and set up the entire test environment again. If you had to do this manually, you might lose at least a day. In addition, there is a chance that you will overlook a small detail and therefore not be able to replicate the test environment exactly again. Initializing a Linux VM with Vagrant or Docker typically only takes a few minutes. Windows systems take a lot more time. The OS itself is a lot bigger\footnote{Approx.~4GB for Windows Server Core compared to 256 to 512MB for a minimal installation of a Linux distribution. Application installations have an additional impact on the disk size of the VM.}

A comprehensive tutorial of Vagrant or Docker is beyond the scope of this guide. An example to illustrate can be found at \url{https://github.com/bertvv/lampstack}. Here an Apache web server is set up with a MariaDB database, PHPMyAdmin, and Wordpress.

\subsection{Reusability}
\label{ssec:reusability}

By automating the entire process, you create an additional advantage: it is a lot easier to create similar variants of a test setup. Suppose you want to measure the performance impact of a cache on a database system. The installation of both variants (with/without cache) are very similar. In a Vagant environment you can create 2 VMs, with the same installation script for the basic setup, and a separate script to realize the different configuration variants.

\section{Conducting experiments and collecting quantitative data}
\label{sec:conductingexpertiments}

When you set up an experiment in which you are going to collect quantitative data, for example for a performance comparison, the same guidelines apply as for a proof-of-concept: it is important that it is reproducible, replicable and reusable. You may start from a predefined test setup, and you will repeatedly perform the same action and take measurements.

Here too it is important to automate the execution of the experiment and to save your test results, for example by means of a shell script. If you have to perform an experiment manually, this quickly becomes a very time-consuming activity. Imagine that every time you want to run the experiment you have to press a button, wait about five to ten minutes and then copy and paste the output to Excel and then edit the spreadsheet again so that you are only left with the numbers to statistically to process. This method is untenable. After all, you have to be constantly present and attentive while performing. Moreover, with every manual operation there is a chance of making mistakes.

A script that can run the experiment in different variants without user interaction, and also immediately saves the results in a suitable file format saves you a lot of time. You can run the experiment overnight and that way you can repeat it enough to get statistically useful results. If an experiment fails, all you need to do is improve the script and run it again. It is also easier to write different variants (like the earlier example of database queries with or without cache).

% TODO:
% - vergelijk je de juiste dingen? Zijn neveneffecten uitgeschakeld?
% - Experimenten vele keren herhalen

% Valkuilen: enquêtes, ...